# -*- coding: utf-8 -*-
"""TD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PCghZe8d-bp5XJeJVpXrq3uzHAciyNYZ
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

data = pd.read_csv('/content/diabetes.csv')

data.sample(5)

X = data.drop(columns = ['Outcome'])
y = data['Outcome']

# Import Libraries
from sklearn.model_selection import train_test_split
#----------------------------------------------------

#----------------------------------------------------
#Splitting data

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=44, shuffle =True)

#Splitted Data
print('X_train shape is ' , X_train.shape)
print('X_test shape is ' , X_test.shape)
print('y_train shape is ' , y_train.shape)
print('y_test shape is ' , y_test.shape)

from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

LR_with_20_80_model= LogisticRegression()

LR_with_20_80_model.fit(X_train, y_train)

y_pred = LR_with_20_80_model.predict(X_test)
y_pred

accuracy_LR_with_20_80_model = accuracy_score(y_test, y_pred)

print(f"accuracy: {accuracy_LR_with_20_80_model:.2f}")

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
# Ø­Ø³Ø§Ø¨ Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ù„ØªØ¨Ø§Ø³
cm = confusion_matrix(y_test, y_pred) # Changed to confusion_matrix

# Ø·Ø¨Ø§Ø¹Ø© Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ù„ØªØ¨Ø§Ø³ Ø¨Ø´ÙƒÙ„ Ù†ØµÙŠ
print("ğŸ”¹ Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ù„ØªØ¨Ø§Ø³:\n", cm)

# Ø±Ø³Ù… Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ù„ØªØ¨Ø§Ø³ Ø¨Ø´ÙƒÙ„ Ù…Ø±Ø¦ÙŠ
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Class 0", "Class 1"], yticklabels=["Class 0", "Class 1"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

# ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØµÙ†ÙŠÙ (Ø§Ù„Ø¯Ù‚Ø©ØŒ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ØŒ F1-score)
print("\nğŸ”¹ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØµÙ†ÙŠÙ:\n", classification_report(y_test, y_pred))





DT_with_20_80_model = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)
DT_with_20_80_model.fit(X_train, y_train)

y_pred = DT_with_20_80_model.predict(X_test)

accuracy_DT_with_20_80_model = accuracy_score(y_test, y_pred)
print(f"ğŸ”¹ Ø¯Ù‚Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {accuracy_DT_with_20_80_model:.2f}")

cm = confusion_matrix(y_test, y_pred)
print("\nğŸ”¹ Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ù„ØªØ¨Ø§Ø³:\n", cm)

plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Class 0", "Class 1"], yticklabels=["Class 0", "Class 1"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - Decision Tree")
plt.show()

print("\nğŸ”¹ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØµÙ†ÙŠÙ:\n", classification_report(y_test, y_pred))

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

X_train

LR_With_StandardScaler = LogisticRegression()
LR_With_StandardScaler.fit(X_train, y_train)

y_pred = LR_With_StandardScaler.predict(X_test)

accuracy_LR_With_StandardScaler = accuracy_score(y_test, y_pred)
print(f" aCCURACY :  {accuracy_LR_With_StandardScaler:.2f}")

cm = confusion_matrix(y_test, y_pred)
print("\nğŸ”¹ Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ù„ØªØ¨Ø§Ø³:\n", cm)


plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Class 0", "Class 1"], yticklabels=["Class 0", "Class 1"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()


print("\nğŸ”¹ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØµÙ†ÙŠÙ:\n", classification_report(y_test, y_pred))



DT_With_StandardScaler = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)

DT_With_StandardScaler.fit(X_train, y_train)

y_pred =DT_With_StandardScaler.predict(X_test)

accuracy_DT_With_StandardScaler = accuracy_score(y_test, y_pred)
print(f"ACCURACY :  {accuracy_DT_With_StandardScaler:.2f}")

cm = confusion_matrix(y_test, y_pred)
print("\nğŸ”¹ confusion_matrix \n", cm)

plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Class 0", "Class 1"], yticklabels=["Class 0", "Class 1"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - Decision Tree")
plt.show()

print("\nğŸ”¹ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØµÙ†ÙŠÙ:\n", classification_report(y_test, y_pred))





# Import Libraries
from sklearn.model_selection import train_test_split
#----------------------------------------------------

#----------------------------------------------------
#Splitting data

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=44, shuffle =True)

#Splitted Data
print('X_train shape is ' , X_train.shape)
print('X_test shape is ' , X_test.shape)
print('y_train shape is ' , y_train.shape)
print('y_test shape is ' , y_test.shape)

X_train

LR_With_30_70_model = LogisticRegression()
LR_With_30_70_model.fit(X_train, y_train)

y_pred = LR_With_30_70_model.predict(X_test)

accuracy_LR_With_30_70_model = accuracy_score(y_test, y_pred)
print(f"accuracy :  {accuracy_LR_With_30_70_model:.2f}")

cm = confusion_matrix(y_test, y_pred)
print("\nğŸ”¹ Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ù„ØªØ¨Ø§Ø³:\n", cm)

plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Class 0", "Class 1"], yticklabels=["Class 0", "Class 1"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

print("\nğŸ”¹ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØµÙ†ÙŠÙ:\n", classification_report(y_test, y_pred))

DT_With_30_70_model = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)

DT_With_30_70_model.fit(X_train, y_train)

y_pred = DT_With_30_70_model.predict(X_test)

accuracy_DT_With_30_70_model = accuracy_score(y_test, y_pred)
print(f"accuracy: {accuracy_DT_With_30_70_model:.2f}")

cm = confusion_matrix(y_test, y_pred)
print("\nğŸ”¹ Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ù„ØªØ¨Ø§Ø³:\n", cm)

plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Class 0", "Class 1"], yticklabels=["Class 0", "Class 1"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - Decision Tree")
plt.show()

print("\nğŸ”¹ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØµÙ†ÙŠÙ:\n", classification_report(y_test, y_pred))

"""logistic L , L2  """

LR_with_l1_model = LogisticRegression(penalty='l1', solver='liblinear', C=1.0)
LR_with_l1_model.fit(X_train, y_train)
y_pred_l1 = LR_with_l1_model.predict(X_test)

LR_with_l2_model = LogisticRegression(penalty='l2', solver='lbfgs', C=1.0)
LR_with_l2_model.fit(X_train, y_train)
y_pred_l2 = LR_with_l2_model.predict(X_test)

accuracy_LR_with_l1_model= accuracy_score(y_test, y_pred_l1)
accuracy_LR_with_l2_model = accuracy_score(y_test, y_pred_l2)

# âœ… Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
print(f"L1 Regularization: {accuracy_LR_with_l1_model:.2f}")
print(f"L2 Regularization: {accuracy_LR_with_l2_model:.2f}")

# Coefficients
print("with L1:", LR_with_l1_model.coef_) # Changed from accuracy_LR_with_l1_model to LR_with_l1_model
print("with L2:", LR_with_l2_model.coef_) # Changed from accuracy_LR_with_l2_model to LR_with_l2_model

fig, axes = plt.subplots(1, 3, figsize=(15, 4))
models = [ ("L1 (Lasso)", y_pred_l1), ("L2 (Ridge)", y_pred_l2)]

for i, (title, y_pred) in enumerate(models):
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Class 0", "Class 1"], yticklabels=["Class 0", "Class 1"], ax=axes[i])
    axes[i].set_title(title)
    axes[i].set_xlabel("Predicted Label")
    axes[i].set_ylabel("True Label")

plt.tight_layout()
plt.show()



"""# **# Finally Result**"""

import pandas as pd

# Ù†ÙØªØ±Ø¶ Ø¥Ù† Ø§Ù„Ù‚ÙŠÙ… Ø¯ÙŠ Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª
data = {
    "Model": [
        "Decision Tree (20/80 split)",
        "Decision Tree (30/70 split)",
        "Decision Tree + StandardScaler",
        "Logistic Regression (20/80 split)",
        "Logistic Regression (30/70 split)",
        "Logistic Regression + StandardScaler",
        "Logistic Regression + L1 Regularization",
        "Logistic Regression + L2 Regularization"
    ],
    "Accuracy": [
        accuracy_DT_with_20_80_model,
        accuracy_DT_With_30_70_model,
        accuracy_DT_With_StandardScaler,
        accuracy_LR_with_20_80_model,
        accuracy_LR_With_30_70_model,
        accuracy_LR_With_StandardScaler,
        accuracy_LR_with_l1_model,
        accuracy_LR_with_l2_model
    ]
}

# ØªØ­ÙˆÙŠÙ„Ù‡Ù… Ù„Ø¬Ø¯ÙˆÙ„ DataFrame
df = pd.DataFrame(data)

# Ø¹Ø±Ø¶ Ø§Ù„Ø¬Ø¯ÙˆÙ„
print(df)



"""# RandomForestClassifier

"""

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

def print_score(clf, X_train, y_train, X_test, y_test, train=True):
    if train:
        pred = clf.predict(X_train)
        clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True))
        print("Train Result:\n================================================")
        print(f"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%")
        print("_______________________________________________")
        print(f"CLASSIFICATION REPORT:\n{clf_report}")
        print("_______________________________________________")
        print(f"Confusion Matrix: \n {confusion_matrix(y_train, pred)}\n")

    elif train==False:
        pred = clf.predict(X_test)
        clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))
        print("Test Result:\n================================================")
        print(f"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%")
        print("_______________________________________________")
        print(f"CLASSIFICATION REPORT:\n{clf_report}")
        print("_______________________________________________")
        print(f"Confusion Matrix: \n {confusion_matrix(y_test, pred)}\n")

from sklearn.ensemble import RandomForestClassifier

rf_clf = RandomForestClassifier(n_estimators=100)
rf_clf.fit(X_train, y_train)

print_score(rf_clf, X_train, y_train, X_test, y_test, train=True)
print_score(rf_clf, X_train, y_train, X_test, y_test, train=False)

"""# DecisionTreeClassifier with GridSearchCV"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier


params = {
    "criterion":("gini", "entropy" ,"log_loss"),
    "splitter":("best", "random"),
    "max_depth":(list(range(1, 20))),
    "min_samples_split":[2, 3, 4 , 5],
    "min_samples_leaf":list(range(1, 30)),
}


tree_clf = DecisionTreeClassifier(random_state=42)
tree_cv = GridSearchCV(tree_clf, params, scoring="accuracy", n_jobs=-1, verbose=1, cv=3)
tree_cv.fit(X_train, y_train)
best_params = tree_cv.best_params_
print(f"Best paramters: {best_params})")

tree_clf = DecisionTreeClassifier(**best_params)
tree_clf.fit(X_train, y_train)
print_score(tree_clf, X_train, y_train, X_test, y_test, train=True)
print_score(tree_clf, X_train, y_train, X_test, y_test, train=False)

"""# other param

"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

params = {
    "criterion": ("gini", "entropy", "log_loss"),
    "splitter": ("best", "random"),
    "max_depth": list(range(1, 20)),
    "min_samples_split": [2, 3, 4, 5],
    "min_samples_leaf": list(range(1, 30)),
    "max_features": ("sqrt", "log2"),  # Ø£Ùˆ Ø¹Ø¯Ø¯ ØµØ­ÙŠØ­ Ø£Ùˆ None
    "max_leaf_nodes": (10, 11, 12),  # Ø£Ùˆ None
    "min_impurity_decrease": (0.01, 0.02, 0.03),
    "class_weight": ("balanced",)  # Ø£Ùˆ {0: 2, 1: 1} Ø£Ùˆ ØºÙŠØ±Ù‡Ø§

}


tree_clf = DecisionTreeClassifier(random_state=42)
tree_cv = GridSearchCV(tree_clf, params, scoring="accuracy", n_jobs=-1, verbose=1, cv=3)
tree_cv.fit(X_train, y_train)
best_params = tree_cv.best_params_
print(f"Best paramters: {best_params})")

tree_clf = DecisionTreeClassifier(**best_params)
tree_clf.fit(X_train, y_train)
print_score(tree_clf, X_train, y_train, X_test, y_test, train=True)
print_score(tree_clf, X_train, y_train, X_test, y_test, train=False)

""" # RandomForestClassifier with RandomizedSearchCV


"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV

n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]
max_depth = [int(x) for x in np.linspace(10, 110, num=11)]
max_depth.append(None)
min_samples_split = [2, 5, 10]
min_samples_leaf = [1, 2, 4]
bootstrap = [True, False]

random_grid = {'n_estimators': n_estimators,
               'max_depth': max_depth, 'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf, 'bootstrap': bootstrap}

rf_clf = RandomForestClassifier(random_state=42)

rf_cv = RandomizedSearchCV(estimator=rf_clf, scoring='f1',param_distributions=random_grid, n_iter=100, cv=3,
                               verbose=2, random_state=42, n_jobs=-1)

rf_cv.fit(X_train, y_train)
rf_best_params = rf_cv.best_params_
print(f"Best paramters: {rf_best_params})")

rf_clf = RandomForestClassifier(**rf_best_params)
rf_clf.fit(X_train, y_train)

print_score(rf_clf, X_train, y_train, X_test, y_test, train=True)
print_score(rf_clf, X_train, y_train, X_test, y_test, train=False)

"""# RandomForestClassifier  with other param



"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]
max_depth = [int(x) for x in np.linspace(10, 110, num=11)]
max_depth.append(None)
min_samples_split = [2, 5, 10]
min_samples_leaf = [1, 2, 4]
bootstrap = [True, False]

# Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
max_features = ['sqrt', 'log2', None, 10, 15]  # Ø£Ùˆ Ø¹Ø¯Ø¯ ØµØ­ÙŠØ­
min_impurity_decrease = [0.0, 0.01, 0.02, 0.03]
criterion = ['gini', 'entropy']
class_weight = ['balanced', None]  # Ø£Ùˆ Ù‚Ø§Ù…ÙˆØ³ Ù…Ø«Ù„ {0: 1, 1: 2}
warm_start = [True, False]
oob_score = [True, False]

# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠ
random_grid = {'n_estimators': n_estimators,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap,
               'max_features': max_features,
               'min_impurity_decrease': min_impurity_decrease,
               'criterion': criterion,
               'class_weight': class_weight,
               'warm_start': warm_start,
               'oob_score': oob_score}


rf_clf = RandomForestClassifier(random_state=42)

rf_cv = RandomizedSearchCV(estimator=rf_clf, scoring='f1',param_distributions=random_grid, n_iter=100, cv=3,
                               verbose=2, random_state=42, n_jobs=-1)

rf_cv.fit(X_train, y_train)
rf_best_params = rf_cv.best_params_
print(f"Best paramters: {rf_best_params})")

rf_clf = RandomForestClassifier(**rf_best_params)
rf_clf.fit(X_train, y_train)

print_score(rf_clf, X_train, y_train, X_test, y_test, train=True)
print_score(rf_clf, X_train, y_train, X_test, y_test, train=False)

"""# Gridsearch with RandomForestClassifier"""

n_estimators = [100, 500, 1000, 1500]
max_depth = [2, 3, 5]
max_depth.append(None)
min_samples_split = [2, 5, 10]
min_samples_leaf = [1, 2, 4, 10]
bootstrap = [True, False]

params_grid = {'n_estimators': n_estimators,
               'max_depth': max_depth, 'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf, 'bootstrap': bootstrap}

rf_clf = RandomForestClassifier(random_state=42)

rf_cv = GridSearchCV(rf_clf, params_grid, scoring="f1", cv=3, verbose=2, n_jobs=-1)


rf_cv.fit(X_train, y_train)
best_params = rf_cv.best_params_
print(f"Best parameters: {best_params}")

rf_clf = RandomForestClassifier(**best_params)
rf_clf.fit(X_train, y_train)

print_score(rf_clf, X_train, y_train, X_test, y_test, train=True)
print_score(rf_clf, X_train, y_train, X_test, y_test, train=False)

"""# Gridsearch with DecisionTreeClassifier



"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

# ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù†Ø·Ø§Ù‚Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ© Ù„Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
criterion = ['gini', 'entropy']
max_depth = [2, 3, 5]
max_depth.append(None)
min_samples_split = [2, 5, 10]
min_samples_leaf = [1, 2, 4, 10]
splitter = ['best', 'random']
class_weight = ['balanced', None]

# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø´Ø¨ÙƒÙŠ
params_grid = {'criterion': criterion,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'splitter': splitter,
               'class_weight': class_weight}

# Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙˆØ¯ÙŠÙ„ DecisionTreeClassifier
dt_clf = DecisionTreeClassifier(random_state=42)

# Ø§Ø³ØªØ®Ø¯Ø§Ù… GridSearchCV Ù„ØªØ­Ø¯ÙŠØ¯ Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
dt_cv = GridSearchCV(dt_clf, params_grid, scoring="f1", cv=3, verbose=2, n_jobs=-1)

# ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… GridSearchCV
dt_cv.fit(X_train, y_train)

# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
best_params = dt_cv.best_params_
print(f"Best parameters: {best_params}")

# Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙˆØ¯ÙŠÙ„ DecisionTreeClassifier Ù…Ø¹ Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
dt_clf = DecisionTreeClassifier(**best_params)
dt_clf.fit(X_train, y_train)

# Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ø§Ø®ØªØ¨Ø§Ø±
print_score(dt_clf, X_train, y_train, X_test, y_test, train=True)
print_score(dt_clf, X_train, y_train, X_test, y_test, train=False)

